{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu5eNQAiQeOyE2MyoAd+Dg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemanthkumarPutta/Network-Traffic-Classification/blob/main/Network_Traffic_Classification%20using%20Deep%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mL08CXdhQAj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Model, Input, Sequential\n",
        "from keras.layers import GRU, LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Activation, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ObiUN8LZjHne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU5yxP2ykMi1",
        "outputId": "d5f25477-e9d3-4c51-b396-8f9ac9deeb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100 # use early stopping\n",
        "FOLDS = 10\n",
        "SEQ_LEN = 25\n",
        "NUM_ROWS = 10000 # just use first day for now, set to -1 for all data\n",
        "MIN_CONNECTIONS_LIST = [100]\n",
        "\n",
        "def read_csv(file_path, has_header=True):\n",
        "    with open(file_path) as f:\n",
        "        if has_header: f.readline()\n",
        "        data = []\n",
        "        for line in f:\n",
        "            line = line.strip().split(\",\")\n",
        "            data.append([x for x in line])\n",
        "    return data"
      ],
      "metadata": {
        "id": "tMICAqs-oIPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_load_and_filter(datasetfile, min_connections):\n",
        "    dataset = read_csv(\"C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\NetworkClassification-master\\\\DL\\\\training\\\\GCseq25.csv\")\n",
        "\n",
        "    # Use first n rows\n",
        "    dataset = dataset[:NUM_ROWS]\n",
        "\n",
        "    # packet sizes\n",
        "    X1 = np.array([z[1:SEQ_LEN + 1] for z in dataset])\n",
        "\n",
        "    # payload sizes\n",
        "    X2 = np.array([z[SEQ_LEN + 1:2*SEQ_LEN + 1] for z in dataset])\n",
        "\n",
        "    # inter-arrival times\n",
        "    X3 = np.array([z[2*SEQ_LEN + 1:3*SEQ_LEN + 1] for z in dataset])\n",
        "    X3 = X3.astype(float)\n",
        "    X3[np.where(X3 != 0 )] = np.log(X3[np.where(X3 != 0 )])\n",
        "\n",
        "    # direction\n",
        "    X4 = np.array([z[3*SEQ_LEN + 1:4*SEQ_LEN + 1] for z in dataset])\n",
        "\n",
        "    y = np.array([z[0] for z in dataset])\n",
        "    print(\"Shape of X1 =\", np.shape(X1))\n",
        "    print(\"Shape of X2 =\", np.shape(X2))\n",
        "    print(\"Shape of X3 =\", np.shape(X3))\n",
        "    print(\"Shape of X4 =\", np.shape(X4))\n",
        "    print(\"Shape of y =\", np.shape(y))\n",
        "\n",
        "    print(\"Entering min connections filter section! \")\n",
        "    snis, counts = np.unique(y, return_counts=True)\n",
        "    above_min_conns = list()\n",
        "\n",
        "    for i in range(len(counts)):\n",
        "        if counts[i] > min_connections:\n",
        "            above_min_conns.append(snis[i])\n",
        "\n",
        "    print(\"Filtering done. SNI classes remaining: \", len(above_min_conns))\n",
        "    indices = np.isin(y, above_min_conns)\n",
        "    X1 = X1[indices]\n",
        "    X2 = X2[indices]\n",
        "    X3 = X3[indices]\n",
        "    X4 = X4[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    print(\"Filtered shape of X1 =\", np.shape(X1))\n",
        "    print(\"Filtered shape of X2 =\", np.shape(X2))\n",
        "    print(\"Filtered shape of X3 =\", np.shape(X3))\n",
        "    print(\"Filtered shape of X4 =\", np.shape(X4))\n",
        "    print(\"Filtered shape of y =\", np.shape(y))\n",
        "\n",
        "    ##### BASIC PARAMETERS #####\n",
        "    n_samples = np.shape(X1)[0]\n",
        "    time_steps = np.shape(X1)[1] # we have a time series of 100 payload sizes\n",
        "    n_features = 1 # 1 feature which is packet size\n",
        "\n",
        "    ##### CREATES MAPPING FROM SNI STRING TO INT #####\n",
        "    class_map = {sni:i for i, sni in enumerate(np.unique(y))}\n",
        "    rev_class_map = {val: key for key, val in class_map.items()}\n",
        "\n",
        "    n_labels = len(class_map)\n",
        "\n",
        "    ##### CHANGE Y TO PD SO ITS EASIER TO MAP #####\n",
        "    y_pd = pd.DataFrame(y)\n",
        "    y_pd = y_pd[0].map(class_map)\n",
        "\n",
        "    ##### DUPLICATE Y LABELS, WE WILL NEED THIS LATER #####\n",
        "    y = y_pd.values.reshape(n_samples,)\n",
        "\n",
        "    return X1, X2, X3, X4, y, time_steps, n_features, n_labels, rev_class_map"
      ],
      "metadata": {
        "id": "ywoU6vD5oj1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DLClassification(X_train, X_test, y_train, y_test,time_steps, n_features, n_labels, dropout):\n",
        "    X_train = np.stack([X_train], axis=2)\n",
        "    X_test = np.stack([X_test], axis=2)\n",
        "\n",
        "    # if you dont have newest keras version, you might have to remove restore_best_weights = True\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min')\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(200, 3, activation='relu', input_shape=(time_steps, n_features)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(400, 3, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GRU(200))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='sigmoid'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(n_labels, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['acc'])\n",
        "    model.summary()\n",
        "    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, shuffle=False, validation_data=(X_test, y_test), callbacks = [early_stopping])\n",
        "    return model.predict(X_test)"
      ],
      "metadata": {
        "id": "N-57DLhJqnWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_sklearn_classification(X_train, X_test, y_train, y_test):\n",
        "  cls = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=300, per_run_time_limit=90, ml_memory_limit=50000)\n",
        "  cls.fit(X_train, y_train)\n",
        "  print(cls.sprint_statistics())\n",
        "  print(cls.show_models())\n",
        "  predictions = cls.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "_s56pRLUS2BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import KFold\n",
        "if __name__ == \"__main__\":\n",
        "    datasetfile = \"C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\NetworkClassification-master\\\\DL\\\\training\\\\GCseq25.csv\"\n",
        "\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True)\n",
        "\n",
        "    # try a variety of min conn settings for graph\n",
        "    accuracies = []\n",
        "    for min_connections in MIN_CONNECTIONS_LIST:\n",
        "        X1, X2, X3, X4, y, time_steps, n_features, n_labels, rev_class_map = data_load_and_filter(datasetfile, min_connections)\n",
        "\n",
        "        total_nn1, total_nn2, total_nn3, total_nn123, total_cls = 0, 0, 0, 0, 0\n",
        "        for train_index, test_index in kf.split(X1):\n",
        "\n",
        "            X1_train, X1_test = X1[train_index], X1[test_index] # Packet sizes\n",
        "            X2_train, X2_test = X2[train_index], X2[test_index] # Payload sizes\n",
        "            X3_train, X3_test = X3[train_index], X3[test_index] # Inter-Arrival Times\n",
        "\n",
        "            # Directional features not used!\n",
        "            # X4_train, X4_test = X4[train_index], X4[test_index]\n",
        "\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            # CNN-RNN for Packet Size\n",
        "            predictions1 = DLClassification(X1_train, X1_test, y_train, y_test, time_steps, n_features, n_labels, dropout=0.0)\n",
        "\n",
        "            # CNN-RNN for Payload Size\n",
        "            predictions2 = DLClassification(X2_train, X2_test, y_train, y_test, time_steps, n_features, n_labels, dropout=0.0)\n",
        "\n",
        "            # CNN-RNN for Inter-Arrival times\n",
        "            predictions3 = DLClassification(X3_train, X3_test, y_train, y_test, time_steps, n_features, n_labels, dropout=0.25)\n",
        "\n",
        "            nn_acc1 = 1. * np.sum([np.argmax(x) for x in predictions1] == y_test) / len(y_test)\n",
        "            print(\"CNN-RNN Packet ACCURACY: %s\"%(nn_acc1))\n",
        "\n",
        "            nn_acc2 = 1. * np.sum([np.argmax(x) for x in predictions2] == y_test) / len(y_test)\n",
        "            print(\"CNN-RNN Payload ACCURACY: %s\"%(nn_acc2))\n",
        "\n",
        "            nn_acc3 = 1. * np.sum([np.argmax(x) for x in predictions3] == y_test) / len(y_test)\n",
        "            print(\"CNN-RNN IAT ACCURACY: %s\"%(nn_acc3))\n",
        "\n",
        "            # Ensemble CNN-RNN\n",
        "            predictions123 = (predictions1 * (1.0/3) + predictions2 * (1.0/3) + predictions3 * (1.0/3))\n",
        "            nn_acc123 = 1. * np.sum([np.argmax(x) for x in predictions123] == y_test) / len(y_test)\n",
        "            print(\"Ensemble CNN-RNN ACCURACY: %s\"%(nn_acc123))\n",
        "\n",
        "            total_nn1+= nn_acc1\n",
        "            total_nn2+= nn_acc2\n",
        "            total_nn3+= nn_acc3\n",
        "            total_nn123+= nn_acc123\n",
        "\n",
        "            # Uncomment for auto sklearn results on sequence features\n",
        "            # cls_acc = auto_sklearn_classification(X_train, X_test, y_train, y_test)\n",
        "            # print(\"Auto sklearn Accuracy: %s \"%(cls_acc))\n",
        "            # total_cls += cls_acc\n",
        "\n",
        "            # Uncomment to run once\n",
        "            # FOLDS = 1\n",
        "            # break\n",
        "\n",
        "        total_nn1 = 1. * total_nn1 / FOLDS\n",
        "        total_nn2 = 1. * total_nn2 / FOLDS\n",
        "        total_nn3 = 1. * total_nn3 / FOLDS\n",
        "        total_nn123 = 1. * total_nn123 / FOLDS\n",
        "        total_cls = 1. * total_cls / FOLDS\n",
        "\n",
        "        print(\"AVG CNN-RNN Packet: %s\\n AVG CNN-RNN Payload: %s\\n AVG CNN-RNN IAT: %s\\n AVG CNN-RNN Ensemble: %s\\n AVG CLS: %s\\n \"%(total_nn1, total_nn2, total_nn3, total_nn123, total_cls))\n",
        "\n",
        "        accuracies.append([total_nn1, total_nn2, total_nn3, total_nn123, total_cls])\n",
        "\n",
        "    print(accuracies)"
      ],
      "metadata": {
        "id": "pspS975ZlDiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}